{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ae5ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium #Q3\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bab3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://images.google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "397df902",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"gLFyf\")\n",
    "designation.send_keys('Cake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0472e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"zgAlFc\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9280916e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (689753778.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\sunit\\AppData\\Local\\Temp\\ipykernel_17784\\689753778.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    if source[0:4] == 'http'):\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    driver.execute_script(\"window.scrolBy(0,1000)\")\n",
    "    \n",
    "images = driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source[0:4] == 'http'):\n",
    "        img_urls.append(source)\n",
    "        \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\sunit\\OneDrive\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087f6d4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3600317080.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\sunit\\AppData\\Local\\Temp\\ipykernel_11936\\3600317080.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Copy code\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    params = {\n",
    "        \"k\": product_name\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "\n",
    "        for product in products:\n",
    "            title = product.find(\"h2\", class_=\"a-size-mini\").text.strip()\n",
    "            price = product.find(\"span\", class_=\"a-offscreen\")\n",
    "            price_text = price.text.strip() if price else \"Price not available\"\n",
    "            \n",
    "            print(\"Product:\", title)\n",
    "            print(\"Price:\", price_text)\n",
    "            print(\"=\" * 50)\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Amazon.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    search_amazon(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def search_flipkart(product_name):\n",
    "    base_url = \"https://www.flipkart.com/search\"\n",
    "    params = {\n",
    "        \"q\": product_name\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "        data = []\n",
    "        for product in products:\n",
    "            try:\n",
    "                brand = product.find(\"div\", class_=\"_4rR01T\").text\n",
    "            except AttributeError:\n",
    "                brand = \"-\"\n",
    "\n",
    "            try:\n",
    "                name = product.find(\"a\", class_=\"IRpwTa\").text\n",
    "            except AttributeError:\n",
    "                name = \"-\"\n",
    "\n",
    "            try:\n",
    "                color = product.find(\"div\", class_=\"_1kwXWO\").text\n",
    "            except AttributeError:\n",
    "                color = \"-\"\n",
    "\n",
    "            try:\n",
    "                details = product.find_all(\"li\", class_=\"rgWa7D\")\n",
    "                ram = details[0].text\n",
    "                storage = details[1].text\n",
    "                primary_camera = details[2].text\n",
    "                secondary_camera = details[3].text\n",
    "                display_size = details[4].text\n",
    "                battery = details[5].text\n",
    "            except IndexError:\n",
    "                ram = \"-\"\n",
    "                storage = \"-\"\n",
    "                primary_camera = \"-\"\n",
    "                secondary_camera = \"-\"\n",
    "                display_size = \"-\"\n",
    "                battery = \"-\"\n",
    "\n",
    "            try:\n",
    "                price = product.find(\"div\", class_=\"_30jeq3\").text\n",
    "            except AttributeError:\n",
    "                price = \"-\"\n",
    "\n",
    "            try:\n",
    "                product_url = \"https://www.flipkart.com\" + product.find(\"a\", class_=\"IRpwTa\")[\"href\"]\n",
    "            except AttributeError:\n",
    "                product_url = \"-\"\n",
    "\n",
    "            data.append([brand, name, color, ram, storage, primary_camera, secondary_camera, display_size, battery, price, product_url])\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\"Brand\", \"Smartphone Name\", \"Colour\", \"RAM\", \"Storage(ROM)\", \"Primary Camera\", \"Secondary Camera\", \"Display Size\", \"Battery Capacity\", \"Price\", \"Product URL\"])\n",
    "        df.to_csv(\"smartphones.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Flipkart.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    search_flipkart(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #5\n",
    "\n",
    "def get_coordinates(api_key, city_name):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\n",
    "        \"address\": city_name,\n",
    "        \"key\": api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if data[\"status\"] == \"OK\":\n",
    "        location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "        latitude = location[\"lat\"]\n",
    "        longitude = location[\"lng\"]\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    google_api_key = \"YOUR_GOOGLE_API_KEY\"\n",
    "    city_to_search = input(\"Enter the city name: \")\n",
    "\n",
    "    latitude, longitude = get_coordinates(google_api_key, city_to_search)\n",
    "\n",
    "    if latitude is not None and longitude is not None:\n",
    "        print(\"Latitude:\", latitude)\n",
    "        print(\"Longitude:\", longitude)\n",
    "    else:\n",
    "        print(\"City not found or API error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e11c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #6\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_laptop_details(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        laptops = soup.find_all(\"div\", class_=\"TopNumbeHeading active sticky-footer\")\n",
    "        \n",
    "        for laptop in laptops:\n",
    "            name = laptop.find(\"div\", class_=\"TopNumbeHeading active sticky-footer\").text\n",
    "            price = laptop.find(\"td\", class_=\"smprice\").text\n",
    "            rating = laptop.find(\"td\", class_=\"smrating\").text\n",
    "            specs = laptop.find_all(\"td\", class_=\"smtd1\")\n",
    "            \n",
    "            # Extract other details here (RAM, storage, processor, etc.)\n",
    "            \n",
    "            print(\"Name:\", name)\n",
    "            print(\"Price:\", price)\n",
    "            print(\"Rating:\", rating)\n",
    "            # Print other details\n",
    "            \n",
    "            print(\"=\" * 50)\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the website.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"URL_TO_THE_DIGIT_PAGE\"  # Replace with the actual URL\n",
    "    scrape_laptop_details(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5316ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #7\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_billionaires_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        billionaires = soup.find_all(\"div\", class_=\"personName\")\n",
    "        \n",
    "        for billionaire in billionaires:\n",
    "            rank = billionaire.find(\"div\", class_=\"rank\").text\n",
    "            name = billionaire.find(\"div\", class_=\"name\").text\n",
    "            net_worth = billionaire.find(\"div\", class_=\"netWorth\").text\n",
    "            age = billionaire.find(\"div\", class_=\"age\").text\n",
    "            citizenship = billionaire.find(\"div\", class_=\"countryOfCitizenship\").text\n",
    "            source = billionaire.find(\"div\", class_=\"source-column\").text\n",
    "            industry = billionaire.find(\"div\", class_=\"category\").text\n",
    "            \n",
    "            print(\"Rank:\", rank)\n",
    "            print(\"Name:\", name)\n",
    "            print(\"Net Worth:\", net_worth)\n",
    "            print(\"Age:\", age)\n",
    "            print(\"Citizenship:\", citizenship)\n",
    "            print(\"Source:\", source)\n",
    "            print(\"Industry:\", industry)\n",
    "            print(\"=\" * 50)\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the website.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"URL_TO_THE_FORBES_PAGE\"  # Replace with the actual URL\n",
    "    scrape_billionaires_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver #8\n",
    "import time\n",
    "\n",
    "def scrape_youtube_comments(url, num_comments=500):\n",
    "    driver = webdriver.Chrome()  # You'll need the ChromeDriver executable\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll to load more comments\n",
    "    while len(driver.find_elements_by_css_selector(\"ytd-comment-renderer\")) < num_comments:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    comments = []\n",
    "\n",
    "    # Extract comments, upvotes, and timestamps\n",
    "    comment_elements = driver.find_elements_by_css_selector(\"ytd-comment-renderer\")\n",
    "    for comment_element in comment_elements:\n",
    "        comment = comment_element.find_element_by_id(\"content\").text\n",
    "        upvotes = comment_element.find_element_by_id(\"vote-count-middle\").text\n",
    "        timestamp = comment_element.find_element_by_css_selector(\"#header-author > yt-formatted-string > a > span\").get_attribute(\"title\")\n",
    "        comments.append({\"comment\": comment, \"upvotes\": upvotes, \"timestamp\": timestamp})\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_url = \"YOUR_YOUTUBE_VIDEO_URL\"\n",
    "    scraped_comments = scrape_youtube_comments(video_url, num_comments=500)\n",
    "\n",
    "    for comment in scraped_comments:\n",
    "        print(\"Comment:\", comment[\"comment\"])\n",
    "        print(\"Upvotes:\", comment[\"upvotes\"])\n",
    "        print(\"Timestamp:\", comment[\"timestamp\"])\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb400b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #9\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostel_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    hostels = soup.find_all(\"div\", class_=\"property-card\")\n",
    "    \n",
    "    data = []\n",
    "    for hostel in hostels:\n",
    "        name = hostel.find(\"h2\", class_=\"title\").text.strip()\n",
    "        distance = hostel.find(\"span\", class_=\"distance\").text.strip()\n",
    "        rating = hostel.find(\"div\", class_=\"score orange big\").text.strip()\n",
    "        total_reviews = hostel.find(\"div\", class_=\"reviews\").text.strip()\n",
    "        overall_reviews = hostel.find(\"div\", class_=\"keyword\", string=\"Overall\").find_next(\"div\").text.strip()\n",
    "        privates_from_price = hostel.find(\"a\", class_=\"price privates from-price\").text.strip()\n",
    "        dorms_from_price = hostel.find(\"a\", class_=\"price dorms from-price\").text.strip()\n",
    "        facilities = \", \".join([item.text.strip() for item in hostel.find_all(\"div\", class_=\"facilities\")])\n",
    "        description = hostel.find(\"div\", class_=\"desc\").text.strip()\n",
    "        \n",
    "        data.append({\n",
    "            \"Name\": name,\n",
    "            \"Distance\": distance,\n",
    "            \"Rating\": rating,\n",
    "            \"Total Reviews\": total_reviews,\n",
    "            \"Overall Reviews\": overall_reviews,\n",
    "            \"Privates From Price\": privates_from_price,\n",
    "            \"Dorms From Price\": dorms_from_price,\n",
    "            \"Facilities\": facilities,\n",
    "            \"Description\": description\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://www.hostelworld.com/search?search_keywords=London, England\"\n",
    "    scraped_data = scrape_hostel_data(base_url)\n",
    "    \n",
    "    for hostel in scraped_data:\n",
    "        print(\"Name:\", hostel[\"Name\"])\n",
    "        print(\"Distance:\", hostel[\"Distance\"])\n",
    "        print(\"Rating:\", hostel[\"Rating\"])\n",
    "        print(\"Total Reviews:\", hostel[\"Total Reviews\"])\n",
    "        print(\"Overall Reviews:\", hostel[\"Overall Reviews\"])\n",
    "        print(\"Privates From Price:\", hostel[\"Privates From Price\"])\n",
    "        print(\"Dorms From Price:\", hostel[\"Dorms From Price\"])\n",
    "        print(\"Facilities:\", hostel[\"Facilities\"])\n",
    "        print(\"Description:\", hostel[\"Description\"])\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d540ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
